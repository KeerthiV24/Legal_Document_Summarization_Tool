{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V4MN149c5PS",
        "outputId": "2956e2a7-a178-4f57-aaaa-0a4276b3275b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.25.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sumy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMWh_LiqdB9h",
        "outputId": "e6265876-c613-4962-d654-2790d5d7a671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sumy in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from sumy) (0.6.2)\n",
            "Requirement already satisfied: breadability>=0.1.20 in /usr/local/lib/python3.11/dist-packages (from sumy) (0.1.20)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from sumy) (2.32.3)\n",
            "Requirement already satisfied: pycountry>=18.2.23 in /usr/local/lib/python3.11/dist-packages (from sumy) (24.6.1)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from sumy) (3.9.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz"
      ],
      "metadata": {
        "id": "PoYqjmW5dEj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyZWzkg8dPC_",
        "outputId": "9382a991-8f1d-4b68-f0b7-13ac09dcdfde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OU_f0TtdTNaK",
        "outputId": "ba3d9f6e-2bb9-40af-fb9a-7ffbcd15a45c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original Document:\n",
            " - 1 -     \n",
            " \n",
            " \n",
            " \n",
            "NC: 2024:KHC:12-DB\n",
            "WP No. 18664 of 2023\n",
            " \n",
            "IN THE HIGH COURT OF KARNATAKA AT BENGALURU \n",
            "DATED THIS THE 2ND DAY OF JANUARY, 2024 \n",
            "PRESENT \n",
            "THE HON'BLE MR PRASANNA B. VARALE, CHIEF JUSTICE \n",
            " AND  \n",
            " THE HON'BLE MR JUSTICE KRISHNA S DIXIT \n",
            "WRIT PETITION NO. 18664 OF 2023 (GM-RES) \n",
            "BETWEEN:  \n",
            " \n",
            "SRI. GURUNATH VADDE, \n",
            "S/O VAJINATH VADDE, \n",
            "AGED ABOUT 59 YEARS, \n",
            "OCC SOCIAL WORKER (SELF EMPLOYEE) \n",
            "RESIDING AT JANASEVA ASHRAMA BHAVANI, \n",
            "BIJAL GAUM, \n",
            "KAMALANAGARA TALUK, \n",
            "BIDAR DISTRICT, \n",
            "PIN CODE NO.-585 443. \n",
            "MOBILE CELL NO. 9901687999 \n",
            "AADHAR CARD NO. 6659 3322 2123, \n",
            "PAN NO. AERPV9730M \n",
            "E-MAIL gurunathvadde@gmail.com \n",
            "…PETITIONER \n",
            " \n",
            "(BY SRI. HANUMANTHAPPA HARAVI B GOUDAR.,ADVOCATE) \n",
            " \n",
            "AND: \n",
            " \n",
            "1. \n",
            "THE PRINCIPAL SECRETARY \n",
            "DEPARTMENT OF INFORMATION  \n",
            "TECHNOLOGY AND BIO-TECHNOLOGY, \n",
            "GOVERNMENT OF KARNATAKA, \n",
            "VIKASA SOUDHA, \n",
            "AMBEDKAR VEEDHI, \n",
            "BENGALURU-560 001. \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Digitally signed\n",
            "by SHARADA\n",
            "VANI B\n",
            "Location: HIGH\n",
            "COURT OF\n",
            "KARNATAKA\n",
            "\n",
            " \n",
            "- 2 -     \n",
            " \n",
            " \n",
            " \n",
            "NC: 2024:KHC:12-DB\n",
            "WP No. 18664 of 2023\n",
            " \n",
            "2. \n",
            "THE DIRECTORATE OF INFORMATION  \n",
            "TECHNOLOGY AND BIO TECHNOLOGY \n",
            "GOVERNMENT OF KARNATAKA, \n",
            "VIKASA SOUDHA, \n",
            "AMBEDKAR VEEDHI, \n",
            "BENGALURU-560 001. \n",
            " \n",
            "3. \n",
            "THE SECRETARY \n",
            "KALYANA KARNATAKA PRADESHA, \n",
            "ABHIVRUDHI MANDALI, \n",
            "KALABURGI-585 326. \n",
            " \n",
            "4. \n",
            "THE DEPUTY COMMISSIONER \n",
            "BIDAR DISTRICT, \n",
            "BIDAR-585 102. \n",
            "…RESPONDENTS \n",
            "(BY SMT.NILOUFER AKBAR.,AGA) \n",
            " \n",
            " \n",
            "THIS WRIT PETITION IS FILED UNDER ARTICLES 226 \n",
            "AND 227 OF THE CONSTITUTION OF INDIA, PRAYING TO \n",
            "i)ISSUE A WRIT IN NATURE OF MANDAMUS DIRECTING THE \n",
            "RESPONDENTS-1 TO 4 FOR ESTABLISHMENT OF INFORMATION \n",
            "TECHNOLOGY AND BIO-TECHNOLOGY PARK IN THE BIDAR \n",
            "DISTRICT AS PER REPRESENTATION DATED 12/06/2023 VIDE \n",
            "ANNEXURE-G WITH IMMEDIATE EFFECT AND ii)ISSUE A WRIT \n",
            "OR ORDER OR DIRECTION. \n",
            "  \n",
            " \n",
            "THIS PETITION COMING ON FOR PRELIMINARY HEARING \n",
            "THIS DAY, CHIEF JUSTICE MADE THE FOLLOWING: \n",
            " \n",
            "ORDER \n",
            " \n",
            " \n",
            "The Petitioner is approaching this Court on the third \n",
            "occasion for the similar relief.  The prayer sought in the \n",
            "Petition reads as under: \n",
            " \n",
            "“It is submitted that the petitioner is seeking \n",
            "only an interim direction to the Respondents \n",
            "shall \n",
            "consider \n",
            "the \n",
            "representation \n",
            "for \n",
            "establishment of Information Technology and \n",
            "\n",
            " \n",
            "- 3 -     \n",
            " \n",
            " \n",
            " \n",
            "NC: 2024:KHC:12-DB\n",
            "WP No. 18664 of 2023\n",
            " \n",
            "Bio-Technology Park in the Bidar District as per \n",
            "recommendations \n",
            "made \n",
            "by \n",
            "the \n",
            "Dr.D.M.Nanjundappa High Power Committee.” \n",
            " \n",
            " \n",
            " \n",
            "2. \n",
            "In 2014, this very Petitioner had approached \n",
            "this Court by filing W.P.No.44740/2014, wherein the \n",
            "prayer sought reads thus: \n",
            " \n",
            "This writ petition is filed under Articles 226 \n",
            "and 227 of the Constitution of India praying to \n",
            "directing \n",
            "the \n",
            "respondents \n",
            "to \n",
            "consider \n",
            "the \n",
            "representation dated 6.6.2014 vide Annexure-C. \n",
            " \n",
            " \n",
            "The said writ petition was disposed off  on recording the \n",
            "submission of learned Principal Government Advocate that \n",
            "provision is made for Hyderabad-Karnataka region under \n",
            "Article \n",
            "371J \n",
            "of \n",
            "the \n",
            "Constitution \n",
            "of \n",
            "India \n",
            "and \n",
            "a  \n",
            "Development Board is already created for the said region, \n",
            "while \n",
            "the \n",
            "claim \n",
            "for \n",
            "establishment \n",
            "of \n",
            "Information \n",
            "Technology Park would be examined, nothing further \n",
            "survived for consideration. \n",
            " \n",
            " \n",
            "3. \n",
            "In 2016 also the Petitioner again approached \n",
            "this Court in W.P.No.15066/2016 with similar prayer \n",
            "namely:  \n",
            "\n",
            " \n",
            "- 4 -     \n",
            " \n",
            " \n",
            " \n",
            "NC: 2024:KHC:12-DB\n",
            "WP No. 18664 of 2023\n",
            " \n",
            "“This Writ petition is filed under Articles 226 and \n",
            "227 of the Constitution of India praying to direct \n",
            "the authorities to consider the representations of \n",
            "the Petitioner  dated 15.07.2015 vide Annexure-\n",
            "B, and etc.”, \n",
            " \n",
            "This Petition also came to be disposed off granting liberty \n",
            "to the Petitioner to make the representation to the \n",
            "Directors of  Information Technology and Bio-Technology, \n",
            "within two weeks.   \n",
            " \n",
            "4. \n",
            "Now, the Petitioner has approached this Court \n",
            "by filing the present writ petition substantially  reiterating \n",
            "his earlier grievance.  The learned counsel vehemently \n",
            "submits that a representation dated 02.06.2023 is filed \n",
            "with the Hon’ble Chief Minister of Karnataka and a copy \n",
            "thereof avails at Annexure-H.  Very interestingly the \n",
            "representation is based on the Report of Dr. D M \n",
            "Nanjundappa.  Learned counsel then invited out attention \n",
            "to the Summary Report placed on record.  On perusal of \n",
            "petition papers it can be said that the report must be a \n",
            "voluminous one because the representation refers to one \n",
            "of the chapters i.e., chapter No.23 alone.  Now at the cost \n",
            "\n",
            " \n",
            "- 5 -     \n",
            " \n",
            " \n",
            " \n",
            "NC: 2024:KHC:12-DB\n",
            "WP No. 18664 of 2023\n",
            " \n",
            "of repetition we state that what is annexed to the petition \n",
            "is only a Summary Report.   \n",
            " \n",
            "5. \n",
            "When we had put a query to the learned \n",
            "counsel for the petitioner as to what material is \n",
            "independently collected by his client who claims to be a \n",
            "social worker in support of his representation,  the only \n",
            "reply was that a report is already submitted by the \n",
            "Committee.  We repeatedly put query to the learned \n",
            "counsel that if any independent information is collected by \n",
            "the Petitioner as t \n",
            "...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "NLTK tokenizers are missing or the language is not supported.\nDownload them by following command: python -c \"import nltk; nltk.download('punkt')\"\nOriginal error was:\n\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sumy/nlp/tokenizers.py\u001b[0m in \u001b[0;36m_get_sentence_tokenizer\u001b[0;34m(self, language)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers/punkt/%s.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLookupError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadZipfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers/punkt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mswitch_punkt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chunkers/maxent_ne_chunker\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mswitch_punkt\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-734831aa4c18>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-734831aa4c18>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nOriginal Document:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n...\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Show first 1000 characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lsa\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSummary:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-734831aa4c18>\u001b[0m in \u001b[0;36msummarize_text\u001b[0;34m(text, num_sentences, method)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msummarize_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lsa\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Summarize text using different algorithms: 'lsa', 'lexrank', 'luhn'.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaintextParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mfitz\u001b[0m  \u001b[0;31m# PyMuPDF for extracting text from PDFs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sumy/nlp/tokenizers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, language)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mtokenizer_language\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLANGUAGE_ALIASES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentence_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sentence_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_word_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_word_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_language\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sumy/nlp/tokenizers.py\u001b[0m in \u001b[0;36m_get_sentence_tokenizer\u001b[0;34m(self, language)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLookupError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadZipfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             raise LookupError(\n\u001b[0m\u001b[1;32m    175\u001b[0m                 \u001b[0;34m\"NLTK tokenizers are missing or the language is not supported.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;34m\"\"\"Download them by following command: python -c \"import nltk; nltk.download('punkt')\"\\n\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: NLTK tokenizers are missing or the language is not supported.\nDownload them by following command: python -c \"import nltk; nltk.download('punkt')\"\nOriginal error was:\n\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def summarize_text(text, num_sentences=5, method=\"lsa\"):\n",
        "    \"\"\"Summarize text using different algorithms: 'lsa', 'lexrank', 'luhn'.\"\"\"\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    import fitz  # PyMuPDF for extracting text from PDFs\n",
        "import nltk\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
        "from sumy.summarizers.luhn import LuhnSummarizer\n",
        "\n",
        "# Ensure NLTK punkt tokenizer is downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text from a given PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as doc:\n",
        "            for page in doc:\n",
        "                text += page.get_text(\"text\") + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text: {e}\")\n",
        "    return text.strip()\n",
        "    if method == \"lsa\":\n",
        "        summarizer = LsaSummarizer()\n",
        "    elif method == \"lexrank\":\n",
        "        summarizer = LexRankSummarizer()\n",
        "    elif method == \"luhn\":\n",
        "        summarizer = LuhnSummarizer()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid summarization method. Choose from 'lsa', 'lexrank', or 'luhn'.\")\n",
        "\n",
        "    summary = summarizer(parser.document, num_sentences)\n",
        "    return \" \".join(str(sentence) for sentence in summary)\n",
        "\n",
        "def main():\n",
        "    pdf_path = \"court.pdf\"  # Change to your PDF file path\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    if not text:\n",
        "        print(\"No text extracted. Please check the PDF file.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nOriginal Document:\\n\", text[:5000], \"\\n...\")  # Show first 1000 characters\n",
        "    summary = summarize_text(text, num_sentences=5, method=\"lsa\")\n",
        "\n",
        "    print(\"\\nSummary:\\n\", summary)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}